\chapter{Introduction}

The music industry plays a significant role in todayâ€™s globalized world as it has been growing rapidly over the years. It also faces the unique challenge of addressing diverse audiences across
different countries. Each country has different tastes in music which are shaped
by lots of factors. For a record label, it is crucial to understand these preferences for a successful music promotion and marketing, for example to decide which song of an album should become a single.\\ The ability to predict
the popularity of a song before it is released can provide a significant 
competitive privilege.\\
More on the code can be found on the GitHub repository at the following \href{https://github.com/noemichem/Data-mining-project}{link}.\\

\section{Objective}
This project has two main goals: one is to predict the popularity of a song based on its audio features, and the other is to identify the most important audio features that contribute to a song's popularity in different countries.\\
In particular, specific audio features have been taken in consideration,  
such as danceability, key, loudness, mode, speechiness and acousticness. 
By leveraging Data Mining and Machine Learning Techniques, this project will provide insights into
the factors determining the popularity of songs in different countries.\\

\section{Dataset}
The final dataset used in this project is obtained by combining two different and extensive datasets.

The first dataset was taken from the following Kaggle \href{https://www.kaggle.com/datasets/dhruvildave/spotify-charts/data} {link}. \\

This dataset is called Spotify Charts and contains information about the most popular songs in different countries. It is a complete dataset 
of all the "Top 200" and "Viral 50" charts published globally by Spotify. Spotify publishes a new chart
 every 2-3 days. This is Spotify's entire collection since January 1, 2017.
 The included key details are the song's name, artist, spotify id, rank, and the number of streams 
 in each country. This dataset is helpful to undertand how musical preferences change across different
 parts of the world. \\

 The second dataset was created by individuating all the unique song IDs from the first dataset and using the Spotify API to gather the audio features of more than 200K songs. The audio features that are used in this project are $danceability, energy, key, loudness, mode, speechiness, acousticness$, 
 $instrumentalness, liveness, valence, tempo, duration (ms), time signature$.  
 These features are essential for analyzing the musical elements that contribute to a song's popularity in each country.\\

 Before starting the preprocessing phase, the two datasets were merged togethere by the Spotify ID. This ensured that the audio features from the Spotify API were 
 correctly associated with the corresponding songs and their popularity metrics in different countries.\\


\newpage
 \section{Audio Features}

 \begin{table}[ht]
    \centering
    \resizebox{\textwidth}{!}{ % Resize table to fit within text width
    \begin{tabular}{|>{\raggedright\arraybackslash}p{0.15\textwidth}|>{\raggedright\arraybackslash}p{0.12\textwidth}|>{\raggedright\arraybackslash}p{0.73\textwidth}|}
    \hline
    \textbf{Feature} & \textbf{Data Type} & \textbf{Description} \\
    \hline
    Danceability & float & Describes how suitable a track is for dancing based on a combination of musical elements. A value of 0.0 is least danceable and 1.0 is most danceable. \\
    \hline
    Energy & float & Measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. \\
    \hline
    Key & integer & The key the track is in. Integers map to pitches using standard Pitch Class notation. If no key was detected, the value is -1. \\
    \hline
    Loudness & float & The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. It is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 dB. \\
    \hline
    Mode & integer & Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0. \\
    \hline
    Speechiness & float & Speechiness detects the presence of spoken words in a track. Speech-like the recording (e.g. audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. \\
    \hline
    Acousticness & float & A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic. \\
    \hline
    Instrumentalness & float & Predicts whether a track contains no vocals. "Ooh" and "aah" sounds are treated as instrumental in this context. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0. \\
    \hline
    Liveness & float & Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live. \\
    \hline
    Valence & float & A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive, while tracks with low valence sound more negative. \\
    \hline
    Tempo & float & The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration. \\
    \hline
    Duration (ms) & integer & The duration of the track in milliseconds. \\
    \hline
    Time Signature & integer & An estimated time signature. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of "3/4", to "7/4". \\
    \hline  
    \end{tabular}
    }
\end{table}