\chapter{Introduction}

\section{Purpose of the project}
The music industry plays a significant role in todayâ€™s globalized world.
It has been growing rapidly over the years. On the other hand, the music
industry faces the unique challenge of addressing divers audiences across
different countries. Each country has different tastes in music which are shaped
by lots of factors. For record labels, understanding these preferences is
crucial for successful music promotion and marketing. The ability to predict
the popularity of a song before it is released can provide a significant 
competitive privilege.\\
\\
This project aims to analyze the varying musical tastes across different countries by 
understanding their popular songs and what makes those songs popular. To be able to understand
which factors make the song more listened than the others in a specific country, the audio features 
such as danceability, key, loudness, mode, speechiness and acousticness of the songs will be analyzed.
By leveraging Data Mining and Machine Learning Techniques, the project will provide insights into
the factors determining the popularity of songs in different countries.\\


\section{Dataset}
The final dataset used in this project is obtained by combining two different and extensive datasets.
The first dataset is taken from the Spotify API. The dataset contains audio features of 160k+ songs. The audio features that are used in this project are $danceability, energy, key, loudness, mode, speechiness, acousticness$, 
$instrumentalness, liveness, valence, tempo, duration (ms), time signature$.  
These features are essential for analyzing the musical elements that contribute to a song's popularity in each country.\\
The second dataset is taken from Kaggle which is called Spotify charts. \\
(https://www.kaggle.com/datasets/dhruvildave/spotify-charts/data)
This dataset contains information about the most popular songs in the countries. It is a complete dataset 
of all the "Top 200" and "Viral 50" charts published globally by Spotify. Spotify publishes a new chart
 every 2-3 days. This is Spotify's entire collection since January 1, 2017.
 The included key details are the song's name, artist, spotify id, rank, and the number of streams 
 in each country. This dataset is helpful to undertand how musical preferences change across different
 parts of the world. \\
 To create a unified dataset, first preprocessing is performed on both datasets, and final versions of two different 
 datasets are combined by matching spotify ids column. This ensured that the audio features from the Spotify API were 
 correctly associated with the corresponding songs and their popularity metrics in different countries.\\
 The final dataset comprises 195000 rows of data in total for 69 different countries. The combined
 dataset is approximately 30 MB in size, making it both manageable for analysis and rich enough to uncover 
 meaningful insights.\\

 \section{Audio Features}

 \begin{table}[ht]
    \centering
    \resizebox{\textwidth}{!}{ % Resize table to fit within text width
    \begin{tabular}{|>{\raggedright\arraybackslash}p{0.2\textwidth}|>{\raggedright\arraybackslash}p{0.2\textwidth}|>{\raggedright\arraybackslash}p{0.6\textwidth}|}
    \hline
    \textbf{Feature} & \textbf{Data Type} & \textbf{Description} \\
    \hline
    Danceability & float & Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable. \\
    \hline
    Energy & float & Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy. \\
    \hline
    Key & integer & The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C$\sharp$/D$\flat$, 2 = D, and so on. If no key was detected, the value is -1. \\
    \hline
    Loudness & float & The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 dB. \\
    \hline
    Mode & integer & Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0. \\
    \hline
    Speechiness & float & Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. \\
    \hline
    Acousticness & float & A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic. \\
    \hline
    Instrumentalness & float & Predicts whether a track contains no vocals. "Ooh" and "aah" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly "vocal". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0. \\
    \hline
    Liveness & float & Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live. \\
    \hline
    Valence & float & A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry). \\
    \hline
    Tempo & float & The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration. \\
    \hline
    Duration (ms) & integer & The duration of the track in milliseconds. \\
    \hline
    Time Signature & integer & An estimated time signature. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of "3/4", to "7/4". \\
    \hline
    \end{tabular}
    }
\end{table}