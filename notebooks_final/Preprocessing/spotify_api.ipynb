{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the dataset charts.csv take all the unique ids of the songs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique track IDs have been saved to 'unique_track_ids.csv'.\n"
     ]
    }
   ],
   "source": [
    "#Take the ids from the url: substitute the url column with the id of the song\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r'C:\\Users\\noemi\\Documents\\GitHub\\Data-mining-project\\datasets_final\\charts.csv')\n",
    "# Extract track_id from the URL\n",
    "def extract_track_id(url):\n",
    "    return url.rstrip('/').split('/')[-1]\n",
    "\n",
    "# Replace the 'url' column with 'track_id'\n",
    "df['url'] = df['url'].apply(extract_track_id)\n",
    "\n",
    "df.rename(columns={'url': 'track_id'}, inplace=True)\n",
    "\n",
    "#Save the track id column inside the same dataset\n",
    "df.to_csv(r'C:\\Users\\noemi\\Documents\\GitHub\\Data-mining-project\\datasets_final\\charts.csv', index=False)\n",
    "\n",
    "# Drop duplicates \n",
    "df_unique = df.drop_duplicates(subset=['track_id'])\n",
    "\n",
    "# Select only the 'track_id' column\n",
    "df_unique_track_ids = df_unique[['track_id']]\n",
    "\n",
    "# Save the unique track IDs to a new CSV file\n",
    "df_unique_track_ids.to_csv(r'C:\\Users\\noemi\\Documents\\GitHub\\Data-mining-project\\datasets_final\\unique_ids.csv', index=False)\n",
    "\n",
    "print(\"Unique track IDs have been saved to 'unique_track_ids.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the unique ids in 2 different files: first 100K and the rest\n",
    "# Load the unique track IDs\n",
    "df = pd.read_csv(r'C:\\Users\\noemi\\Documents\\GitHub\\Data-mining-project\\datasets_final\\unique_ids.csv')\n",
    "\n",
    "# Divide il DataFrame\n",
    "df_1 = df.head(100000)  # First 100.000 elements\n",
    "df_2 = df.tail(len(df) - 100000)  # The rest\n",
    "\n",
    "# Save the DataFrame\n",
    "df_1.to_csv(r'C:\\Users\\noemi\\Documents\\GitHub\\Data-mining-project\\datasets_final\\unique_track_ids_first_100K.csv', index=False)\n",
    "df_2.to_csv(r'C:\\Users\\noemi\\Documents\\GitHub\\Data-mining-project\\datasets_final\\unique_track_ids_last_100K.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Spotify API to get the audio features of the songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import spotipy.exceptions as exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = 'a27ad4424c9840b69b7115ae6a35f9be'\n",
    "CLIENT_SECRET = '63ebebf7fbed486694babc106fbc2ff8'\n",
    "\n",
    "# Authentication\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=CLIENT_ID, client_secret=CLIENT_SECRET)\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n",
    "\n",
    "def get_audio_features(track_ids):\n",
    "    try:\n",
    "        audio_features = sp.audio_features(tracks=track_ids)\n",
    "        return audio_features\n",
    "    except exceptions.SpotifyException as e:\n",
    "        print(f\"Error while getting audio features: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the DataFrame from CSV\n",
    "    track_ids_df = pd.read_csv(r'C:\\Users\\noemi\\Documents\\GitHub\\Data-mining-project\\datasets_final\\unique_track_ids_first_100K.csv')\n",
    "\n",
    "    # Check if the file has the 'track_id' column\n",
    "    if 'track_id' not in track_ids_df.columns:\n",
    "        print(\"CSV file doesn't have the column 'track_id'.\")\n",
    "    else:\n",
    "        # Initialize empty DataFrame to store audio features\n",
    "        features_df = pd.DataFrame()\n",
    "\n",
    "        # Process track IDs in batches of 100\n",
    "        for i in range(0, len(track_ids_df), 100):\n",
    "            # Extract the 'track_id' column for the current batch\n",
    "            batch_track_ids = track_ids_df['track_id'].iloc[i:i+100].tolist()\n",
    "\n",
    "            # Get audio features for the current batch\n",
    "            features_list = get_audio_features(batch_track_ids)\n",
    "\n",
    "            if features_list:\n",
    "                # Filter only the desired columns\n",
    "                columns = [\n",
    "                    'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', \n",
    "                    'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', \n",
    "                    'type', 'id', 'uri', 'track_href', 'analysis_url', 'duration_ms', \n",
    "                    'time_signature'\n",
    "                ]\n",
    "\n",
    "                # Create a list of dictionaries with the features\n",
    "                filtered_features_list = []\n",
    "                for features in features_list:\n",
    "                    if features:\n",
    "                        filtered_features = {key: features[key] for key in columns}\n",
    "                        filtered_features_list.append(filtered_features)\n",
    "\n",
    "                # Create a DataFrame with the audio features for the current batch\n",
    "                batch_features_df = pd.DataFrame(filtered_features_list)\n",
    "\n",
    "                # Append the current batch to the main DataFrame\n",
    "                features_df = pd.concat([features_df, batch_features_df], ignore_index=True)\n",
    "\n",
    "        # Save the DataFrame to a new CSV file\n",
    "        features_df.to_csv(r'C:\\Users\\noemi\\Documents\\GitHub\\Data-mining-project\\datasets_final\\first_100K_track_ids_features.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = 'a27ad4424c9840b69b7115ae6a35f9be'\n",
    "CLIENT_SECRET = '63ebebf7fbed486694babc106fbc2ff8'\n",
    "\n",
    "# Authentication\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=CLIENT_ID, client_secret=CLIENT_SECRET)\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n",
    "\n",
    "def get_audio_features(track_ids):\n",
    "    try:\n",
    "        audio_features = sp.audio_features(tracks=track_ids)\n",
    "        return audio_features\n",
    "    except exceptions.SpotifyException as e:\n",
    "        print(f\"Error while getting audio features: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the DataFrame from CSV\n",
    "    track_ids_df = pd.read_csv(r'C:\\Users\\noemi\\Documents\\GitHub\\Data-mining-project\\datasets_final\\unique_track_ids_last_100K.csv')\n",
    "\n",
    "    # Check if the file has the 'track_id' column\n",
    "    if 'track_id' not in track_ids_df.columns:\n",
    "        print(\"CSV file doesn't have the column 'track_id'.\")\n",
    "    else:\n",
    "        # Initialize empty DataFrame to store audio features\n",
    "        features_df = pd.DataFrame()\n",
    "\n",
    "        # Process track IDs in batches of 100\n",
    "        for i in range(0, len(track_ids_df), 100):\n",
    "            # Extract the 'track_id' column for the current batch\n",
    "            batch_track_ids = track_ids_df['track_id'].iloc[i:i+100].tolist()\n",
    "\n",
    "            # Get audio features for the current batch\n",
    "            features_list = get_audio_features(batch_track_ids)\n",
    "\n",
    "            if features_list:\n",
    "                # Filter only the desired columns\n",
    "                columns = [\n",
    "                    'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', \n",
    "                    'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', \n",
    "                    'type', 'id', 'uri', 'track_href', 'analysis_url', 'duration_ms', \n",
    "                    'time_signature'\n",
    "                ]\n",
    "\n",
    "                # Create a list of dictionaries with the features\n",
    "                filtered_features_list = []\n",
    "                for features in features_list:\n",
    "                    if features:\n",
    "                        filtered_features = {key: features[key] for key in columns}\n",
    "                        filtered_features_list.append(filtered_features)\n",
    "\n",
    "                # Create a DataFrame with the audio features for the current batch\n",
    "                batch_features_df = pd.DataFrame(filtered_features_list)\n",
    "\n",
    "                # Append the current batch to the main DataFrame\n",
    "                features_df = pd.concat([features_df, batch_features_df], ignore_index=True)\n",
    "\n",
    "        # Save the DataFrame to a new CSV file\n",
    "        features_df.to_csv(r'C:\\Users\\noemi\\Documents\\GitHub\\Data-mining-project\\datasets_final\\last_100K_track_ids_features.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the 2 files in one\n",
    "# Load the DataFrames\n",
    "df_1 = pd.read_csv(r'C:\\Users\\noemi\\Documents\\GitHub\\Data-mining-project\\datasets_final\\first_100K_track_ids_features.csv')\n",
    "df_2 = pd.read_csv(r'C:\\Users\\noemi\\Documents\\GitHub\\Data-mining-project\\datasets_final\\last_100K_track_ids_features.csv')\n",
    "\n",
    "# Merge the DataFrames\n",
    "df = pd.concat([df_1, df_2], ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "df.to_csv(r'C:\\Users\\noemi\\Documents\\GitHub\\Data-mining-project\\datasets_final\\track_ids_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rank</th>\n",
       "      <th>date</th>\n",
       "      <th>artist</th>\n",
       "      <th>track_id</th>\n",
       "      <th>region</th>\n",
       "      <th>chart</th>\n",
       "      <th>trend</th>\n",
       "      <th>streams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chantaje (feat. Maluma)</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>6mICuAdrwEjh6Y6lroV2Kg</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>top200</td>\n",
       "      <td>SAME_POSITION</td>\n",
       "      <td>253019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vente Pa' Ca (feat. Maluma)</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Ricky Martin</td>\n",
       "      <td>7DM4BPaS7uofFul3ywMe46</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>top200</td>\n",
       "      <td>MOVE_UP</td>\n",
       "      <td>223988.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reggaetón Lento (Bailemos)</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>CNCO</td>\n",
       "      <td>3AEZUABDXNtecAOSC1qTfo</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>top200</td>\n",
       "      <td>MOVE_DOWN</td>\n",
       "      <td>210943.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Safari</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>J Balvin, Pharrell Williams, BIA, Sky</td>\n",
       "      <td>6rQSrBHf7HlZjtcMZ4S4bO</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>top200</td>\n",
       "      <td>SAME_POSITION</td>\n",
       "      <td>173865.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shaky Shaky</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Daddy Yankee</td>\n",
       "      <td>58IL315gMSTD37DOZPJ2hf</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>top200</td>\n",
       "      <td>MOVE_UP</td>\n",
       "      <td>153956.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         title  rank        date  \\\n",
       "0      Chantaje (feat. Maluma)     1  2017-01-01   \n",
       "1  Vente Pa' Ca (feat. Maluma)     2  2017-01-01   \n",
       "2   Reggaetón Lento (Bailemos)     3  2017-01-01   \n",
       "3                       Safari     4  2017-01-01   \n",
       "4                  Shaky Shaky     5  2017-01-01   \n",
       "\n",
       "                                  artist                track_id     region  \\\n",
       "0                                Shakira  6mICuAdrwEjh6Y6lroV2Kg  Argentina   \n",
       "1                           Ricky Martin  7DM4BPaS7uofFul3ywMe46  Argentina   \n",
       "2                                   CNCO  3AEZUABDXNtecAOSC1qTfo  Argentina   \n",
       "3  J Balvin, Pharrell Williams, BIA, Sky  6rQSrBHf7HlZjtcMZ4S4bO  Argentina   \n",
       "4                           Daddy Yankee  58IL315gMSTD37DOZPJ2hf  Argentina   \n",
       "\n",
       "    chart          trend   streams  \n",
       "0  top200  SAME_POSITION  253019.0  \n",
       "1  top200        MOVE_UP  223988.0  \n",
       "2  top200      MOVE_DOWN  210943.0  \n",
       "3  top200  SAME_POSITION  173865.0  \n",
       "4  top200        MOVE_UP  153956.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_charts = pd.read_csv(r'C:\\Users\\noemi\\Documents\\GitHub\\Data-mining-project\\datasets_final\\charts.csv')\n",
    "\n",
    "df_charts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the track_ids_features.csv rename id to track_id and save it in the same file\n",
    "df = pd.read_csv(r'C:\\Users\\noemi\\Documents\\GitHub\\Data-mining-project\\datasets_final\\track_ids_features.csv')\n",
    "\n",
    "# Rename the 'id' column to 'track_id'\n",
    "df.rename(columns={'id': 'track_id'}, inplace=True)\n",
    "\n",
    "# Save the DataFrame to the same CSV file\n",
    "df.to_csv(r'C:\\Users\\noemi\\Documents\\GitHub\\Data-mining-project\\datasets_final\\track_ids_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the audio features with the dataset charts.csv\n",
    "# Load the DataFrames\n",
    "df_charts = pd.read_csv(r'C:\\Users\\noemi\\Documents\\GitHub\\Data-mining-project\\datasets_final\\charts.csv')\n",
    "df_features = pd.read_csv(r'C:\\Users\\noemi\\Documents\\GitHub\\Data-mining-project\\datasets_final\\track_ids_features.csv')\n",
    "\n",
    "# Merge the DataFrames\n",
    "df_merged = pd.merge(df_charts, df_features, on='track_id', how='inner')\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "df_merged.to_csv(r'C:\\Users\\noemi\\Documents\\GitHub\\Data-mining-project\\datasets_final\\charts_with_features.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
